{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TME 7 Chaine de Markov Caché\n",
    "### Annotation de gènes par chaînes de Markov Caché\n",
    "\n",
    "Les modeles de chaines de Markov caché sont tres utilisées notament dans les domaines de la reconnaisance de la parole, du traitement automatique du langage naturel, de la reconnaissance de l'écriture manuscrite et de la bioinformatique.\n",
    "\n",
    "Les 3 problèmes de bases des HMM (*Hidden Markov Model*) sont :\n",
    "1. Évaluation : \n",
    " -  Problème : calculer la probabilité d’observation de la séquence d’observations étant donnée un HMM:  \n",
    " -  Solution : *Forward Algorithm *\n",
    "\n",
    "2. Décodage : \n",
    " - Problème : trouver la séquence d’états qui maximise la séquence d’observations \n",
    " - Solution : *Viterbi Algorithm *\n",
    "\n",
    "3. Entraînement :  \n",
    " - Problème : ajuster les paramètres du modèle HMM afin de maximiser la probabilité de générer une séquence d’observations à partir de données d’entraînement  \n",
    "  - Solution : *Forward-Backward Algorithm*\n",
    "  \n",
    "\n",
    "Dans ce TME, nous allons appliquer l'algorithme Viterbi à des données biologiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rappel de biologie\n",
    "\n",
    "Dans ce TME, nous allons voir comment les modèles statistiques peuvent être utilisés pour extraire de l'information des données biologiques brutes. Le but sera de spécifier des modèles de Markov cachées qui permettent d'annoter les positions des gènes dans le génome.\n",
    "\n",
    "Le génome, support de l'information génétique, peut être vu comme une longue séquence de caractères écrite dans un alphabet à 4 lettres: A , C , G et T . Chaque lettre du génome est aussi appelée pair de base (ou bp). Il est maintenant relativement peu coûteux de séquencer un génome (quelques milliers d'euros pour un génome humain). Cependant on ne peut pas comprendre, simplement à partir de la suite de lettres, comment cette information est utilisée par la cellule (un peu comme avoir à disposition un manuel d'instructions écrit dans une langue inconnue).\n",
    "\n",
    "Un élément essentiel est le gène, qui après transcription et traduction produira les protéines, les molécules responsables de la grande partie de l'activité biochimique des cellules.\n",
    "\n",
    "La traduction en protéine est faite à l'aide du code génétique qui, à chaque groupe de 3 lettres (ou bp) transcrites fait correspondre un acide aminé. Ces groupes de 3 lettres sont appelés codon et il y en a $4^3$, soit $64$. Donc, en première approximation, un gène est défini par les propriétés suivantes (pour les organismes procaryotes):\n",
    "\n",
    "- Le premier codon, appelé codon start est ATG,\n",
    "- Il y a 61 codons qui codent pour la séquence d'acides aminés.\n",
    "- Le dernier codon, appelé codon stop, marque la fin du gène et est l'une des trois séquences TAA , TAG ou TGA . Il n'apparaît pas dans le gène. \n",
    "\n",
    "\n",
    "Nous allons intégrer ces différents éléments d'information pour prédire les positions des gènes. Notez que pour simplifier nous avons omis le fait que la molécule d'ADN est constituée de deux brins complémentaires, et donc que les gènes présents sur le brin complémentaire sont vus \"à l'envers\" sur notre séquence. Les régions entre les gènes sont appelées les régions intergéniques ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://cdn.kastatic.org/ka-perseus-images/1ade7bbd40ca8dbc7a55ddf4067935e42c347f35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chacune des séquences de gènes commence par un codon start et fini par un des codons stop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation de gènes \n",
    "## Question 1 : Téléchargement des données\n",
    "\n",
    "Nous travaillerons sur le premier million de bp du génome de E. coli (souche 042). Plutôt que de travailler avec les lettres A , C , G et T , nous allons les recoder avec des numéros ($A =0$, $C=1$, $G=2$, $T =3$). \n",
    "\n",
    "Les annotations fournies sont également codées de $0$ à $3$ :\n",
    "- 0 si la position est dans une region non codante = region intergenique\n",
    "- 1 si la position correspond a la position 0 d'un codon\n",
    "- 2 si la position correspond a la position 1 d'un codon\n",
    "- 3 si la position correspond a la position 2 d'un codon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telechargez le fichier et ouvrez le avec pickle\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "Genome=np.load('genome.npy') # Le premier million de bp de E. coli\n",
    "Annotation=np.load('annotation.npy')# L'annotation sur le genome\n",
    "\n",
    "## On divise nos donnees, la moitie va nous sevir pour l'apprentissage du modèle\n",
    "## l'autre partie pour son evaluation\n",
    "\n",
    "genome_train=Genome[:500000]\n",
    "genome_test=Genome[500000:]\n",
    "\n",
    "annotation_train=Annotation[:500000]\n",
    "annotation_test=Annotation[500000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : Apprentissage \n",
    "\n",
    "Comme modèle le plus simple pour séparer les séquences de codons des séquences intergéniques, on va définir la chaîne de Markov caché dont le graphe de transition est donné ci dessous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](http://mapsi.lip6.fr/uploads/Cours/modele1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tel modèle se défini de la manière suivante : nous considerons qu'il existe 4 états cachés possibles (intergénique, condon 0, codon 1, condon 2).\n",
    "\n",
    "On peut rester dans les régions intergéniques, et quand on démarre un gène, la composition de chaque base du codon est différente. Il va falloir, afin de pouvoir utiliser ce modèle pour classifier, connaître les paramètres pour la matrice de transition (donc ici uniquement les probas $a$ et $b$), et les lois $(b_i,i=0,…,3)$ des observations pour les quatre états.\n",
    "\n",
    "```python\n",
    "Pi = np.array([1, 0, 0, 0])  ##on commence dans l'intergenique\n",
    "A =  np.array([[1-a, a  , 0, 0], \n",
    "              [0  , 0  , 1, 0],\n",
    "              [0  , 0  , 0, 1],\n",
    "              [b  , 1-b, 0, 0 ]])\n",
    "B = ...\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donnée la structure d'un HMM (Hidden Markov Chain):\n",
    "\n",
    "- les observations n'influencent pas les états: les matrices $\\Pi$ (distribution de probabilité initiale), $A$ (matrice de transition) s'obtiennent comme dans un modèle de Markov simple (cf semaine 6)\n",
    "- chaque observation ne dépend que de l'état courant \n",
    "\n",
    "La nature des données nous pousse à considérer des lois de probabilités discrètes quelconques pour les émissions. L'idée est donc de procéder par comptage en définissant la matrice $B$ (matrice de probabilités des émissions) comme suit:\n",
    "\n",
    "- $K$ colonnes (nombre d'observations), $N$ lignes (nombre d'états)\n",
    "- Chaque ligne correspond à une loi d'émission pour un état (ie, chaque ligne somme à 1) \n",
    "\n",
    "Ce qui donne l'algorithme:\n",
    "\n",
    "1. $b_{ij}$ = comptage des émissions depuis l'état $s_i$ vers l'observation $x_j$\n",
    "2. normalisation des lignes de $B$ \n",
    "\n",
    "Donner le code de la fonction `def learnHMM(allX, allS, N, K):` qui apprend un modèle à partir d'un ensemble de couples (seq. d'observations, seq. d'états) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnHMM(allx, allq, N, K):\n",
    "    \"\"\" apprend un modèle à partir \n",
    "    d'un ensemble de couples (seq. d'observations, seq. d'états) \n",
    "    retourne les matrices A  B \"\"\"\n",
    "    A = np.zeros((N, N)) \n",
    "    B = np.zeros((N, K)) \n",
    "\n",
    "    ### votre code \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi = np.array([1, 0, 0, 0])\n",
    "nb_etat= 4 ## (intergénique, condon 0, codon 1, condon 2)\n",
    "nb_observation = 4 ## (A,T,C,G)\n",
    "# A,B =learnHMM(genome_train, annotation_train, nb_etat, nb_observation)\n",
    "# print(A)\n",
    "# print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez trouver \n",
    "\n",
    "$A= $\n",
    "```python\n",
    "[[0.99899016 0.00100984 0.         0.        ]\n",
    " [0.         0.         1.         0.        ]\n",
    " [0.         0.         0.         1.        ]\n",
    " [0.00272284 0.99727716 0.         0.        ]]\n",
    "```\n",
    "$B=$       \n",
    "```python\n",
    "[[0.2434762  0.25247178 0.24800145 0.25605057]\n",
    " [0.24727716 0.23681872 0.34909315 0.16681097]\n",
    " [0.28462222 0.23058695 0.20782446 0.27696637]\n",
    " [0.1857911  0.26246354 0.29707437 0.25467098]]\n",
    "```\n",
    "        \n",
    "Notez que ce sont des matrices de probabilites, la somme de chaque ligne donne 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 :  Estimation la séquence d'états par Viterbi\n",
    "\n",
    "Il n'est pas toujours évident de trouver les régions codante et non codante d'un genome. Nous souhaiterions annoter automatiquement le génome, c'est-à-dire retrouver **la séquence d'états cachés la plus probable** ayant permis de générer la séquence d'observation.\n",
    "\n",
    "### Rappels sur l'algorithme Viterbi (1967):\n",
    "\n",
    "- Il sert à estimer la séquence d'états la plus probable étant donnés les observations et le modèle.\n",
    "- Il peut servir à approximer la probabilité de la séquence d'observation étant donné le modèle. \n",
    "\n",
    "1\\. Initialisation (avec les indices à 0 en python): \n",
    "\n",
    "$$\\begin{array}{ccccccccc} \\delta_{0} (i) &=& \\log \\pi_{i} +\\log b_{i} (x_{0}) \\\\ \\Psi_{0}(i) &=& -1 \\end{array}$$\n",
    " Note: L'initialisation de $\\Psi_0(i)$ à $-1$ car $-1$ n'est pas utilisé normalement (n'est pas un état valide).\n",
    " \n",
    "2\\. Récursion: \n",
    "\n",
    "$$ \\begin{array}{ccccccccc} \\delta_{t} (j) &=&\\displaystyle \\left[\\max_{i} \\delta_{t-1}(i) + \\log a_{ij}\\right] + \\log b_{j}(x_{t}) \\\\ \\Psi_{t}(j) &=&\\displaystyle \\arg\\max_{i\\in [1,\\ N]} \\delta_{t-1} (i) + \\log a_{ij} \\end{array}$$\n",
    "\n",
    "3\\. Terminaison (indices à {$T-1$} en python) \n",
    "\n",
    "$$ S^{\\star} = \\max_{i} \\delta_{T-1}(i)$$\n",
    "\n",
    "4\\. Chemin $$\\begin{array}{ccccccccc} s_{T-1}^{\\star} & = &\\displaystyle \\arg\\max_{i} \\delta_{T-1}(i) \\\\ s_{t}^{\\star} & = & \\displaystyle \\Psi_{t+1}(s_{t+1}^{\\star}) \\end{array}$$\n",
    "\n",
    "L'estimation de $\\log p(x_0^{T-1} | \\lambda)$ est obtenue en cherchant la plus grande probabilité dans la dernière colonne de $\\delta$. Donner le code de la méthode `viterbi(x,Pi,A,B):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(allx,Pi,A,B):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    allx : array (T,)\n",
    "        Sequence d'observations.\n",
    "    Pi: array, (K,)\n",
    "        Distribution de probabilite initiale\n",
    "    A : array (K, K)\n",
    "        Matrice de transition\n",
    "    B : array (K, M)\n",
    "        Matrice d'emission matrix\n",
    "\n",
    "    \"\"\"\n",
    "    ## initialisation\n",
    "    psi = np.zeros((len(A), len(allx))) # A = N\n",
    "    psi[:,0]= -1\n",
    "    delta = np.zeros((len(A), len(allx)))  # initialisation en dimension mais pas en contenu !\n",
    "    \n",
    "    ## recursion ...  (votre code )\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etat_predits=viterbi(genome_test,Pi,A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage\n",
    "On met les états cachés soit à $0$ (**non codant**) soit à $1$ (**codant**).\n",
    "```python\n",
    "etat_predits[etat_predits!=0]=1 \n",
    "annotation_test[annotation_test!=0]=1\n",
    "```\n",
    "puis on affiche pour position du génome si c'est une position codante ou non en utilisant les vrais annotations, puis on affiche pour chaque position si elle est predite comme codante ou non.\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=(15,2))\n",
    "ax.plot(annotation_test, label=\"annotation\", lw=3, color=\"black\", alpha=.4)\n",
    "ax.plot(etat_predits, label=\"prediction\", ls=\"--\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez considérer une sous partie du génome, par exemple entre 100000 et 200000. Commentez vos observations sur la qualité de la prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 :  Evaluation des performances\n",
    "\n",
    "À partir des prédictions et des vrai annotations du génome, dessiner la matrice de confusion. \n",
    "\n",
    "<div  align=\"left\"><img src=\"confusion.png\" width=\"200\"></div>\n",
    "\n",
    "Avec : \n",
    "- TP = True Positives, les régions codantes correctement prédictes,\n",
    "- FP = False Positives, les régions intergénique prédites comme des régions codantes,\n",
    "- TN = True Negatives, les régions intergeniques prédites correctement,\n",
    "- FN = False Negatives, les régions codantes prédites comme non codantes.\n",
    "\n",
    "L'état **non codant** est l'état $0$, les autres ($1,2,3$) sont les états **codants**.\n",
    "\n",
    "![](conf2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(true_sequence, predicted_sequence):\n",
    "    ## votre code\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir créé la matrice de confusion, vous pouvez l'afficher en utilisant :\n",
    "\n",
    "```python\n",
    "\n",
    "mat_conf=create_confusion_matrix(annotation_test, etat_predits)\n",
    "plt.imshow(mat_conf)\n",
    "plt.colorbar()\n",
    "ax = plt.gca();\n",
    "\n",
    "# Major ticks\n",
    "ax.set_xticks(np.arange(0, 2, 1));\n",
    "ax.set_yticks(np.arange(0, 2, 1));\n",
    "\n",
    "# Labels for major ticks\n",
    "ax.set_xticklabels(['codant','intergenique']);\n",
    "ax.set_yticklabels(['regions predites comme codantes','regions predites comme non codantes']);\n",
    "\n",
    "print(mat_conf)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donner une interprétation.\n",
    "Peut-on utiliser ce modèle pour prédire la position des gènes dans le génome ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP,FP=mat_conf[0] \n",
    "# FN,TN=mat_conf[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 :  Génération de nouvelles séquences\n",
    "\n",
    "En utilisant le modèle $ \\lambda=\\{Pi,A,B\\}$, créer ` create_seq(N,Pi,A,B) ` une fonction permettant de générer :\n",
    "- une séquence d'états cachés\n",
    "- une sequence d'observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(N,Pi,A,B):\n",
    "    '''\n",
    "    Produire N états cachés en utilisant Pi et A\n",
    "    \n",
    "    et pour chaque état cachés produire une observation en utilisant B\n",
    "    '''\n",
    "    ## votre code\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 : Construction d'un nouveau modèle\n",
    "\n",
    "Évaluons maintenant si cela s'améliore en prenant en compte les frontières des gènes en construisant un modèle avec codon start et codon stop.\n",
    "On veut maintenant d'intégrer l'information complémentaire qui dit qu'un gène commence \"toujours\" par un codon start et finit \"toujours\" par un codon stop avec le graphe de transition ci-dessous.\n",
    "\n",
    "On considère donc maintenant un modèle à 12 états cachés.\n",
    "![](modele2.png)\n",
    "\n",
    "\n",
    "- Écrivez la matrice de transition correspondante, en mettant les probabilités de transition entre lettres pour les codons stop à 0.5.\n",
    "\n",
    "\n",
    "- Adaptez la matrice des émissions pour tous les états du modèle. Vous pouvez réutiliser la matrice B, calculée précédement. Les états correspondant au codons stop n'émettrons qu'une seule lettre avec une probabilité $1$.\n",
    "Pour le codon start, on sait que les proportions sont les suivantes: \n",
    "\n",
    "    - ATG : 83%, \n",
    "    - GTG: 14%,     \n",
    "    - TTG: 3%\n",
    "\n",
    "```python\n",
    "Pi2 = np.array(   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ])  ##on commence encore dans l'intergenique\n",
    "A2 =  np.array([[1-a, a, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ],\n",
    "                [0  , 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 ],\n",
    "                  ... ])\n",
    "B2 = ...\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évaluez les performances du nouveau modèle en faisant de nouvelles predictions d'annotation pour genome_test, et comparez les avec le modèle précédent. \n",
    "```\n",
    "etat_predits2=viterbi(genome_test,Pi2,A2,B2)\n",
    "etat_predits2[etat_predits2!=0]=1 \n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,2))\n",
    "ax.plot(annotation_test, label=\"annotation\", lw=3, color=\"black\", alpha=.4)\n",
    "ax.plot(etat_predits, label=\"prediction model1\", ls=\"--\")\n",
    "ax.plot(etat_predits2, label=\"prediction model2\", ls=\"--\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer la matrice de confusion avec les nouvelles prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
