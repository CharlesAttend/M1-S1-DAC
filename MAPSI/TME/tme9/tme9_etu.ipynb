{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression & optimisation par descente de gradient\n",
    "\n",
    "Ce tme a deux objectifs: \n",
    " - acquérir les connaissances de base pour faire face au problème de la régression, c'est à dire de l'estimation d'un score réel correpondant à une situation,\n",
    " - travailler sur les techniques d'optimisation par descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de données jouet & construction d'une solution analytique\n",
    "\n",
    "Dans un premier temps, générons des données jouets paramétriques:\n",
    " - $N$: nombre de points à générer\n",
    " - $x \\in [0, 1]$ tirage avec un simple rand() ou un linspace() -au choix-\n",
    "     - Si vous optez pour un tirage aléatoire des abscisses, triez les points pour simplifier les traitements ultérieurs\n",
    " - $y=ax+b+\\epsilon, \\epsilon \\sim \\mathcal N(0,\\sigma^2)$\n",
    "     - Rappel : en multipliant un tirage aléatoire selon une gaussienne centrée réduite par $\\sigma$ on obtient le bruit décrit ci-dessus\n",
    "\n",
    "Afin de travailler sur les bonnes pratiques, nous distinguerons un ensemble d'apprentissage et un ensemble de test.\n",
    "Les deux sont tirés selon la même distribution. L'ensemble de test comptera -arbitrairement- 1000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_lin(a, b, sig, N=500, Ntest=1000):\n",
    "    X_train = np.sort(np.random.rand(N)) # sort optionnel, mais ça aide pour les plots\n",
    "    X_test  = \n",
    "    y_train =\n",
    "    y_test  = \n",
    "    #A compléter\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# génération de données jouets:\n",
    "a = 6.\n",
    "b = -1.\n",
    "N = 100\n",
    "sig = .4 # écart type\n",
    "\n",
    "X_train, y_train, X_test, y_test = gen_data_lin(a, b, sig, N)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_test, y_test, 'r.')\n",
    "plt.plot(X_train, y_train, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez obtenir quelque chose de la forme:\n",
    "![données jouet](fig/gen_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation des formules analytiques\n",
    "Nous avons vu deux types de résolutions analytique: à partir des estimateurs des espérances et co-variances d'une part et des moindres carrés d'autre part. Testons les deux méthodes.\n",
    "\n",
    "### Estimation de paramètres probabilistes\n",
    " - $\\hat a = \\frac{\\mbox{cov}(X,Y)}{\\sigma_x^2}$\n",
    " - $\\hat b = E(Y)−\\frac{\\mbox{cov}(X,Y)}{\\sigma_x^2} E(X)$\n",
    " \n",
    " Estimer les paramètres, calculer l'erreur au sens des moindres carrés sur les données d'apprentissage et de test, puis tracer la droite de régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modele_lin_analytique(X_train, y_train):\n",
    "    # A compléter\n",
    "    return ahat, bhat\n",
    "\n",
    "ahat, bhat = modele_lin_analytique(X_train, y_train)\n",
    "print(ahat, bhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erreur_mc(y, yhat):\n",
    "    return ((y-yhat)**2).mean()\n",
    "\n",
    "yhat_train = ahat*X_train+bhat\n",
    "yhat_test  = ahat*X_test+bhat\n",
    "\n",
    "print('Erreur moyenne au sens des moindres carrés (train):', erreur_mc(yhat_train, y_train))\n",
    "print('Erreur moyenne au sens des moindres carrés (test):', erreur_mc(yhat_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X_test, y_test, 'r.')\n",
    "plt.plot(X_train, y_train, 'b')\n",
    "plt.plot(X_test, yhat_test, 'g', lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation au sens des moindres carrés\n",
    "\n",
    "Nous partons directement sur une écriture matricielle. Du coup, il est nécessaire de construire la matrice enrichie $Xe$:\n",
    "    $$Xe = \\left[\\begin{array}{cc}\n",
    "                X_0 & 1\\\\\n",
    "                \\vdots & \\vdots\\\\\n",
    "                X_N & 1\n",
    "                \\end{array}\n",
    "                \\right] $$\n",
    "Le code de la fonction d'enrichissement est donné ci-dessous.\n",
    "\n",
    "Il faut ensuite poser et résoudre un système d'équations linéaires de la forme:\n",
    "$$ A w = B $$\n",
    "**Rappel des formules vues en cours/TD:**\n",
    "$$ A=X^T X$$\n",
    "$$ B=X^T Y$$\n",
    "Fonction de résolution: `np.linalg.solve(A,B)`\n",
    "Vous devez obtenir la même solution que précédemment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mat_lin_biais(X): # fonctionne pour un vecteur unidimensionel X\n",
    "    N = len(X)\n",
    "    return np.hstack((X.reshape(N,1),np.ones((N,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = make_mat_lin_biais(X_train)\n",
    "# A compléter\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit les données polynomiales générées avec la fonction ci-dessous\n",
    " - proposer & une solution d'enrichissement (vue en cours et TD) \n",
    " - résoudre analytiquement le problème des moindres carrés\n",
    " - calculer l'erreur au sens des moindes carrés en apprentissage ET en test\n",
    " - tracer les données et la solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_poly2(a, b, c, sig, N=500, Ntest=1000):\n",
    "    '''\n",
    "    Tire N points X aléatoirement entre 0 et 1 et génère y = ax^2 + bx + c + eps\n",
    "    eps ~ N(0, sig^2)\n",
    "    '''\n",
    "    X_train = np.sort(np.random.rand(N))\n",
    "    X_test  = np.sort(np.random.rand(Ntest))\n",
    "    y_train = a*X_train**2+b*X_train+c+np.random.randn(N)*sig\n",
    "    y_test  = a*X_test**2 +b*X_test +c+np.random.randn(Ntest)*sig\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "Xp_train, yp_train, Xp_test, yp_test = gen_data_poly2(10, -10, 5, 0.1, N=100, Ntest=100)\n",
    "plt.figure()\n",
    "plt.plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mat_poly_biais(X): # fonctionne pour un vecteur unidimensionel X\n",
    "    # A compléter\n",
    "\n",
    "Xe   = make_mat_poly_biais(X_train)\n",
    "Xe_t = make_mat_poly_biais(X_test)\n",
    "w    = # A compléter \n",
    "\n",
    "yhat   = # A compléter\n",
    "yhat_t = # A compléter\n",
    "print('Erreur moyenne au sens des moindres carrés (train):', erreur_mc(yhat, y_train))\n",
    "print('Erreur moyenne au sens des moindres carrés (test):', erreur_mc(yhat_t, y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_train, y_train, 'b.')\n",
    "plt.plot(X_train, yhat, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de coût & optimisation par descente de gradient\n",
    "\n",
    "Comme vu en TD et en cours, nous allons maintenant résoudre le problème de la régression par minimisation d'une fonction de coût:\n",
    "$$ C = \\sum_{i=1}^N (y_i - f(x_i))$$\n",
    "\n",
    "Soit un problème avec des données $(x_i,y_i)_{i=1,\\ldots,N}$, une fonction de décision/prédiction paramétrée par un vecteur $w$ et une fonction de cout à optimiser $C(w)$.\n",
    "Notre but est de trouver les paramètres $w^\\star$ minimisant la fonction de coût:\n",
    "$$ w^\\star = \\arg\\min_w C(w)$$\n",
    "\n",
    "l'algorithme de la descente de gradient est le suivant (rappel):\n",
    "\n",
    " - $w_0 \\leftarrow init$ par exemple : 0\n",
    " - boucle\n",
    "     - $w_{t+1} \\leftarrow w_{t} - \\epsilon \\nabla_w C(w_t)$\n",
    "\n",
    "Compléter le squelette d'implémentation fourni ci-dessous:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour travailler en matrice: (re)construction de la matrice contenant les X et un biais\n",
    "Xe = make_mat_lin_biais(X_train) # dataset linéaire, transformation lineaire des données\n",
    "wstar = # A compléter # pour se rappeler du w optimal\n",
    "\n",
    "def descente_grad_mc(X, y, eps=1e-4, nIterations=100):\n",
    "    w = np.zeros(X.shape[1]) # init à 0\n",
    "    allw = [w]\n",
    "    for i in range(nIterations):\n",
    "        # A COMPLETER => calcul du gradient vu en TD\n",
    "        #\n",
    "        allw.append(w) # stockage de toutes les valeurs intermédiaires pour analyse\n",
    "    allw = np.array(allw)\n",
    "    return w, allw # la dernière valeur (meilleure) + tout l'historique pour le plot\n",
    "    \n",
    "w, allw = descente_grad_mc(Xe, y_train, eps=1e-4, nIterations=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse ensuite à comprendre la descente de gradient dans l'espace des paramètres. Le code ci-dessous permet de tracer le cout pour un ensemble de paramètres (toutes les valeurs de paramètres prises par l'algorithmes au fil du temps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracer de l'espace des couts\n",
    "def plot_parametres( allw, X, y, opti = [], ngrid = 20, extract_bornes=False):\n",
    "    '''\n",
    "    Fonction de tracer d'un historique de coefficients\n",
    "    ATTENTION: ca ne marche qu'en 2D (évidemment)\n",
    "    Chaque w doit contenir 2 valeurs\n",
    "    \n",
    "    Il faut fournir les données (X,y) pour calculer le cout associé \n",
    "    à un jeu de paramètres w\n",
    "    ATTENTION X = forme matricielle des données\n",
    "    '''\n",
    "    w_min = [-0.5, -2] # bornes par défaut, uniquement pour notre cas d'usage\n",
    "    w_max = [8, 5]\n",
    "    if extract_bornes: # bornes générales\n",
    "        w_min = np.min(allw,0) # trouver les bornes\n",
    "        w_max = np.max(allw,0)\n",
    "    # faire une grille régulière avec tous les couples possibles entre le min et le max\n",
    "    w1range = np.linspace(w_min[0], w_max[0], ngrid)\n",
    "    w2range = np.linspace(w_min[1], w_max[1], ngrid)\n",
    "    w1,w2 = np.meshgrid(w1range,w2range)\n",
    "    #\n",
    "    # calcul de tous les couts associés à tous les couples de paramètres\n",
    "    cost = np.array([[np.log(((X @ np.array([w1i,w2j])-y)**2).sum()) for w1i in w1range] for w2j in w2range])\n",
    "    #\n",
    "    plt.figure()\n",
    "    plt.contour(w1, w2, cost)\n",
    "    if len(opti) > 0:\n",
    "        plt.scatter(wstar[0], wstar[1],c='r')\n",
    "    plt.plot(allw[:,0],allw[:,1],'b+-' ,lw=2 )\n",
    "    return\n",
    "    \n",
    "plot_parametres( allw, Xe, y_train, opti=wstar)\n",
    "# plt.savefig('fig/grad_descente.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez obtenir un image de la forme :\n",
    "![Descente de gradient](fig/grad_descente.png)\n",
    "\n",
    "Tester différents jeux de paramètres pour mettre en évidence les phénomènes suivants:\n",
    " - Divergence du gradient\n",
    " - Convergence incomplète (trop lente ou pas assez d'itération)\n",
    " - Convergence idéale: pas de gradient suffisamment grand et nombre d'itérations bien choisi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passage sur des données réelles\n",
    "\n",
    "Après avoir étudié trois manières de faire face au problème de la régression, nous proposons d'étudier un cas réel: la prédiction de la consommation des voitures en fonction de leurs caractéristiques.\n",
    "\n",
    "Dans le cas présent, nous allons baser la solution sur la résolution analytique du problème des moindres carrés (`np.linalg.solve(A,B)`), qui semble la mieux adaptée au problème qui nous intéresse.\n",
    "\n",
    "Le jeu de données est issu des datasets UCI, un répertoire parmi les plus connus en machine learning. Les données **sont déjà téléchargées et présentes dans le tme** mais vous voulez plus d'informations:\n",
    "https://archive.ics.uci.edu/ml/datasets/auto+mpg\n",
    "\n",
    "![voiture](fig/Large9.jpg)\n",
    "\n",
    "Après avoir importé les données (fonction fournie), vous construirez une solution optimale et l'évaluerez au sens des moindres carrés en apprentissage et en test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Chargement des données\n",
    "data = pd.read_csv('data/auto-mpg.data', delimiter='\\s+', header=None) # comme np.loadtxt mais en plus robuste\n",
    "# remplacement des données manquantes '?' => Nan pour travailler sur des nombres\n",
    "data.iloc[:,[3]] = data.iloc[:,[3]].replace('?', None)\n",
    "data.iloc[:,[3]] = data.iloc[:,[3]].astype(float)\n",
    "# remplacement des valeurs manquantes par la moyenne\n",
    "data.iloc[:,[3]] = data.iloc[:,[3]].fillna(data.iloc[:,[3]].mean())\n",
    "\n",
    "print(data.head()) # visualiser ce qu'il y a dans les données\n",
    "\n",
    "X = np.array(data.values[:,1:-2], dtype=np.float64)\n",
    "y = np.array(data.values[:,0], dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separartion app/test\n",
    "def separation_train_test(X, y, pc_train=0.75):\n",
    "    # A compléter\n",
    "    # 1) générer tous les index entre 0 et N-1\n",
    "    # 2) mélanger la liste\n",
    "    napp = int(len(y)*pc_train) # nb de points pour le train\n",
    "    X_train, y_train = X[index[:napp]], y[index[:napp]]\n",
    "    X_test, y_test   = X[index[napp:]], y[index[napp:]]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = separation_train_test(X, y, pc_train=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résolution analytique\n",
    "\n",
    "w = # A compléter\n",
    "yhat   = # A compléter\n",
    "yhat_t = # A compléter\n",
    "print('Erreur moyenne au sens des moindres carrés (train):', erreur_mc(yhat, y_train))\n",
    "print('Erreur moyenne au sens des moindres carrés (test):', erreur_mc(yhat_t, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y(y_train, y_test, yhat, yhat_t):\n",
    "    # tracé des prédictions:\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    plt.plot(y_test, label=\"GT\")\n",
    "    plt.plot(yhat_t, label=\"pred\")\n",
    "    plt.title('En test')\n",
    "    plt.legend()\n",
    "    plt.subplot(212)\n",
    "    plt.plot(y_train, label=\"GT\")\n",
    "    plt.plot(yhat, label=\"pred\")\n",
    "    plt.title('En train')\n",
    "    return\n",
    "plot_y(y_train, y_test, yhat, yhat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interprétation des poids\n",
    "plt.figure()\n",
    "plt.bar(np.arange(len(w)), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "Sur le diagramme ci-dessus, on ne voit pas grand chose pour une raison évidente: on ne peut pas comparer ces poids qui comparent des variables dont les ordres de grandeur sont différents.\n",
    "\n",
    "Nous allons donc assimiler chaque colonne $X_j$ à une variable suivant une loi normale et nous allons revenir à une Normale centrée réduite selon la formule de base:\n",
    "\n",
    "$$X_j \\sim \\mathcal N(\\mu_j, \\sigma_j^2) $$\n",
    "$$\\Rightarrow Z_j = \\frac{X_j - \\mu_j}{\\sigma_j} \\sim \\mathcal N(0, 1) $$\n",
    "\n",
    "Tous les $Z_j$ sont comparables et nous seront en mesure de comprendre l'impact de chaque variables sur les résultats.\n",
    "\n",
    "**ATTENTION:** on ne se basera que sur les données d'apprentissage pour le calcul des $\\{\\mu_j, \\sigma_j\\}$.\n",
    "\n",
    "Une fois la normalisation effectuée, analyser l'impact des différentes variables descriptives sur la valeur à prédire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(X_train, X_test):\n",
    "    '''\n",
    "    Fonction de normalisation des données pour rendre les colonnes comparables\n",
    "    Chaque variable sera assimilée à une loi normale qu'il faut centrer + réduire.\n",
    "    ATTENTION: il faut calculer les moyennes et écarts-types sur les données d'apprentissage seulement\n",
    "    '''\n",
    "    # A compléter\n",
    "    # 1) calcul des moyennes et écarts types pour chaque colonne\n",
    "    # 2) normalisation des colonnes\n",
    "    # 3) Ajout d'un biais: fourni ci-dessous)\n",
    "    Xn_train = np.hstack((Xn_train, np.ones((Xn_train.shape[0], 1))))\n",
    "    Xn_test   = np.hstack((Xn_test, np.ones((X_test.shape[0], 1))))\n",
    "    return Xn_train, Xn_test\n",
    "\n",
    "Xn_train, Xn_test = normalisation(X_train, X_test)\n",
    "w = # A compléter\n",
    "\n",
    "yhat   = # A compléter\n",
    "yhat_t = # A compléter\n",
    "print('Erreur moyenne au sens des moindres carrés (train):', erreur_mc(yhat, y_train))\n",
    "print('Erreur moyenne au sens des moindres carrés (test):', erreur_mc(yhat_t, y_test))\n",
    "plot_y(y_train, y_test, yhat, yhat_t)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(len(w)), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions d'ouverture\n",
    "\n",
    "## Sélection de caractéristiques\n",
    "\n",
    "Quels sont les résultats obtenus en éliminant toutes les variables servent moins?\n",
    "\n",
    "## Feature engineering\n",
    "\n",
    "En étudiant la signification des variables du problèmes, on trouve:\n",
    "\n",
    "1. mpg: continuous \n",
    "2. cylinders: multi-valued discrete \n",
    "3. displacement: continuous \n",
    "4. horsepower: continuous \n",
    "5. weight: continuous \n",
    "6. acceleration: continuous \n",
    "7. model year: multi-valued discrete \n",
    "8. origin: multi-valued discrete \n",
    "\n",
    "D'après la question précédente, le poids, l'année du modèle et le biais sont des facteurs important pour le calcul de la consommation... Jusqu'ici, nous n'avons pas pris en compte l'origine qui était difficile à coder.\n",
    "\n",
    "### Encodage de l'origine\n",
    "\n",
    "La variable origine est accessible de la manière suivante:\n",
    "\n",
    "```\n",
    "  origine = data.values[:,-2]\n",
    "```\n",
    "Il faut le faire au début du traitement pour bien conserver la séparation en l'apprentissage et le test.\n",
    "\n",
    "Au moins les deux derniers facteurs discrets pourraient être traités différemment en one-hot encoding:\n",
    "$$X_j = x \\in \\{1, \\ldots, K\\} \\Rightarrow [0, 0, 1, 0] \\in \\{0, 1\\}^K$$\n",
    "\n",
    "La valeur $x$ donne l'index de la colonne non nulle.\n",
    "\n",
    "### Encodage de l'année\n",
    "\n",
    "Pour l'année, il est possible de procéder de la même manière, mais il préférable de découper les années en 10 catégories puis d'encoder pour limiter le nombre de dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question d'ouverture sur le gradient\n",
    "\n",
    "## La normalisation a-t-elle un impact sur le gradient?\n",
    "\n",
    "La normalisation des données peut au moins nous aider à régler plus facilement le pas (qui sera toujours du même ordre de grandeur... Mais cela a-t-il un impact sur la manière dont nous nous rapprochons de la solution optimale?\n",
    "\n",
    "## Gradient stochastique\n",
    "\n",
    "Dans la plupart des algorithmes modernes d'optimisation liés aux réseaux de neurones, le gradient est calculé de manière stochastique, sur un exemple à la fois:\n",
    "\n",
    "- $w_0 \\leftarrow init$ par exemple : 0\n",
    "- boucle\n",
    "     - tirage d'une donnée $i$: $(x_i,y_i)$\n",
    "     - $w_{t+1} \\leftarrow w_{t} - \\epsilon \\nabla_w C_i(w)$\n",
    "\n",
    "\n",
    "Etudier le fonctionnement de cet algorithme sur les exemples jouets précédents.\n",
    "\n",
    "## Amélioration du gradient\n",
    "\n",
    "Le blog de S. Ruder explique particulièrement bien les améliorations possibles sur les descentes de gradient.\n",
    "\n",
    "https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "Comparer une descente de gradient stochastique avec et sans moment sur les données jouets des premières questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
